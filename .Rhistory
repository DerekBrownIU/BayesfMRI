a <- matrix(rnorm(10), 5, 2)
tmp <- list(a=a, b=a)
sapply(tmp, class)
sapply(tmp, dim)
library(roxygen2)
roxygenize()
any(sapply(tmp, class) != 'matrix')
sapply(template_mean, dim)[1,]
sapply(tmp, dim)[1,]
dim(a)
V=5
L=2
nvox=5
paste0('Each element of template_mean must be an LxV (L=',L,', V=',nvox,') matrix')
cat(paste0('Length of timeseries: T = ',ntime,'\n'))
ntime=4
cat(paste0('Length of timeseries: T = ',ntime,'\n'))
cat(paste0('Number of voxels/vertices: V = ',nvox,'\n'))
cat(paste0('Number of ICs: L = ',ntime,'\n'))
#check that the number of data locations (nvox), time points (ntime) and ICs (L) makes sense
if(ntime > nvox) warning('More time points than voxels. Are you sure?')
if(L > nvox) stop('The arguments you supplied suggest that you want to estimate more ICs than you have data locations.  Please check the orientation and size of template_mean, template_var and BOLD.')
if(L > ntime) stop('The arguments you supplied suggest that you want to estimate more ICs than you have time points.  Please check the orientation and size of template_mean, template_var and BOLD.')
setwd('~/Dropbox/RESEARCH/DiagnosticICA/simulation')
Nx <- 46
Ny <- 55
N <- Nx*Ny #number of voxels
mask <- matrix(1,nrow=Nx,ncol=Ny)
Q <- 3
load(file='template_orig.RData') #template_mean_orig, template_var_orig
dim(template_mean_orig)
q=1
template_mean_q_mat <- mask
template_mean_q_mat[mask==1] <- template_mean_orig[,q]
image(template_mean_q_mat)
c((Nx-shift+1):Nx,1:(Nx-shift))
shift=5
c((Nx-shift+1):Nx,1:(Nx-shift))
length(c((Nx-shift+1):Nx,1:(Nx-shift)))
dim(template_mean_q)
template_mean_q <- mask
dim(template_mean_q)
template_mean_q <- mask
template_mean_q[mask==1] <- template_mean_orig[,q]
template_mean_q_grp2 <- template_mean_q[c((Nx-shift+1):Nx,1:(Nx-shift)),c((Ny-shift+1):Ny,1:(Ny-shift))]
template_mean_q_grp2
image(template_mean_q_grp2)
shift <- 3 #shift in location (pixels)
template_mean_q_grp2 <- template_mean_q[c((Nx-shift+1):Nx,1:(Nx-shift)),c((Ny-shift+1):Ny,1:(Ny-shift))]
image(template_mean_q_grp2)
library(roxygen2)
roxygenize()
library(devtools)
use_build_ignore('R/BayesGLM_group.R')
roxygenize()
roxygenize()
roxygenize()
roxygenize()
setwd('~/Dropbox/RESEARCH/DiagnosticICA/simulation')
shift <- 1:3 #shift in location (pixels)
shift <- 1:3 #shift in location (pixels)
Nx <- 46
Ny <- 55
N <- Nx*Ny #number of voxels
mask <- matrix(1,nrow=Nx,ncol=Ny)
Q <- 3
load(file='template_orig.RData') #template_mean_orig, template_var_orig
sh=shift[1]
template_mean_group2 <- template_mean_orig*0
for(q in 1:Q){
template_mean_q <- mask
template_mean_q[mask==1] <- template_mean_orig[,q]
template_mean_q_grp2 <- template_mean_q[c((Nx-shift+1):Nx,1:(Nx-shift)),c((Ny-shift+1):Ny,1:(Ny-shift))]
template_mean_group2[,q] <- as.vector(template_mean_q_grp2)
}
warnings()
template_mean_group2 <- template_mean_orig*0
for(q in 1:Q){
template_mean_q <- mask
template_mean_q[mask==1] <- template_mean_orig[,q]
template_mean_q_grp2 <- template_mean_q[c((Nx-sh+1):Nx,1:(Nx-sh)),c((Ny-sh+1):Ny,1:(Ny-sh))]
template_mean_group2[,q] <- as.vector(template_mean_q_grp2)
}
image(template_mean_q)
image(template_mean_q_grp2)
sh=3
template_mean_group2 <- template_mean_orig*0
for(q in 1:Q){
template_mean_q <- mask
template_mean_q[mask==1] <- template_mean_orig[,q]
template_mean_q_grp2 <- template_mean_q[c((Nx-sh+1):Nx,1:(Nx-sh)),c((Ny-sh+1):Ny,1:(Ny-sh))]
template_mean_group2[,q] <- as.vector(template_mean_q_grp2)
}
image(template_mean_q_grp2)
#smoothing kernel
FWHM <- 5
sig = FWHM/(2*sqrt(2*log(2)))
load(file='var_corr.RData')
smooth_dev <- function(dev, sigma, var_correction=1){
require(spatstat) #blur
dev_img <- as.im(matrix(dev,nrow=Nx,ncol=Ny)) #reshape to matrix form
dev_img_sm <- blur(dev_img, sigma=sigma) #smooth with Gaussian kernel
dev_img_sm$v <- dev_img_sm$v*var_correction #match variance of unsmoothed image
result <- as.vector(dev_img_sm$v)
return(result)
}
template_mean <- list(group1 = template_mean_orig,
group2 = template_mean_group2)
template_var <- list(group1 = 0.5*abs(template_mean_orig),
group2 = 0.5*abs(template_mean_group2))
# GROUP 1
n <- 100 + 50 #100 training subjects (use to estimate template) + 50 test subjects
subjICs_group1 <- array(NA, dim=c(n, N, Q))
subjICs_group2 <- array(NA, dim=c(n, N, Q))
subjICs <- array(NA, dim=c(n, N, Q))
subjICs <- list(group1 = subjICs, group2 = subjICs)
rm(subjICs_group1)
rm(subjICs_group2)
# GROUP 1
n <- 100 + 50 #100 training subjects (use to estimate template) + 50 test subjects
subjICs <- array(NA, dim=c(n, N, Q))
subjICs <- list(group1 = subjICs, group2 = subjICs)
g=1
ii=1
q=1
dev_iq <- rnorm(N, rep(0,N), sqrt(template_var[[g]][,q])) #generate deviations (subject effects)
dev_iq_sm <- smooth_dev(dev=dev_iq, sigma=sig, var_correction==var_corr[q])
dev_iq_sm <- smooth_dev(dev=dev_iq, sigma=sig, var_correction=var_corr[q])
subjIC_iq <- template_mean[[g]][,q] + dev_iq_sm
subjICs[[g]][ii,,q] <- scale(subjIC_iq, scale=FALSE) #center each IC map
my.image.scale(subjICs[[g]][ii,,q], mask, max=8, min=-2, cols=rainbow(10))
source('sim_funs.R')
my.image.scale(subjICs[[g]][ii,,q], mask, max=8, min=-2, cols=rainbow(10))
my.image.scale(subjICs[[g]][ii,,q], mask, max=8, min=-4, cols=c('turquoise','blue','red','yellow'))
my.image.scale(subjICs[[g]][ii,,q], mask, max=8, min=-4, cols=c('turquoise','darkblue','red','yellow'))
my.image.scale(dev_iq, mask, max=-4, min=-4, cols=c('turquoise','darkblue','yellow'))
summary(dev_iq)
my.image.scale(dev_iq, mask, max=4, min=-4, cols=c('turquoise','darkblue','yellow'))
my.image.scale(dev_iq_sm, mask, max=4, min=-4, cols=c('turquoise','darkblue','red'))
# GROUP 1
n <- 100 + 50 #100 training subjects (use to estimate template) + 50 test subjects
subjICs <- array(NA, dim=c(n, N, Q))
subjICs <- list(group1 = subjICs, group2 = subjICs)
for(g in 1:2){
for(ii in 1:n){
print(ii)
for(q in 1:Q){
dev_iq <- rnorm(N, rep(0,N), sqrt(template_var[[g]][,q])) #generate deviations (subject effects)
dev_iq_sm <- smooth_dev(dev=dev_iq, sigma=sig, var_correction=var_corr[q])
subjIC_iq <- template_mean[[g]][,q] + dev_iq_sm
subjICs[[g]][ii,,q] <- scale(subjIC_iq, scale=FALSE) #center each IC map
#my.image.scale(subjICs[[g]][ii,,q], mask, max=8, min=-4, cols=c('turquoise','darkblue','red','yellow'))
}
}
}
save(subjICs, file = 'subjICs.RData')
load(file = 'subjICs.RData') #subjICs_train
# GROUP 1
n_train <- 100 #100 training subjects (use to estimate template)
n_test <- 50 #50 test subjects
n <- n_train + n_test
subjICs <- array(NA, dim=c(n, N, Q))
subjICs <- list(group1 = subjICs, group2 = subjICs)
for(g in 1:2){
for(ii in 1:n){
print(ii)
for(q in 1:Q){
dev_iq <- rnorm(N, rep(0,N), sqrt(template_var[[g]][,q])) #generate deviations (subject effects)
dev_iq_sm <- smooth_dev(dev=dev_iq, sigma=sig, var_correction=var_corr[q])
subjIC_iq <- template_mean[[g]][,q] + dev_iq_sm
subjICs[[g]][ii,,q] <- scale(subjIC_iq, scale=FALSE) #center each IC map
#my.image.scale(subjICs[[g]][ii,,q], mask, max=8, min=-4, cols=c('turquoise','darkblue','red','yellow'))
}
}
}
save(subjICs, file = 'subjICs.RData')
# RE-ESTIMATE TEMPLATES (MEAN AND VARIANCE)
template_mean2 <- template_mean
template_var2 <- template_var
# RE-ESTIMATE TEMPLATES (MEAN AND VARIANCE)
template_mean2 <- template_mean
template_var2 <- template_var
g=1
template_mean2[[g]] <- apply(subjICs[[g]][1:n_train,], c(2,3), mean)
template_mean2[[g]] <- apply(subjICs[[g]][1:n_train,,], c(2,3), mean)
template_var2[[g]] <- apply(subjICs[[g]][1:n_train,,], c(2,3), var)
for(g in 1:2){
template_mean2[[g]] <- apply(subjICs[[g]][1:n_train,,], c(2,3), mean)
template_var2[[g]] <- apply(subjICs[[g]][1:n_train,,], c(2,3), var)
}
my.image.scale(template_mean2[[g]][,q], mask, max=8, min=-4, cols=c('turquoise','darkblue','red','yellow'))
my.image.scale(template_mean[[g]][,q], mask, max=8, min=0, cols=c('darkblue','red','yellow'))
my.image.scale(template_mean2[[g]][,q], mask, max=8, min=0, cols=c('darkblue','red','yellow'))
getwd()
library(roxygen2)
roxygenize()
library(devtools)
install_github('mandymejia/ciftiTools')
roxygenize()
library(ciftiTools)
library(roxygen2)
roxygenize()
library(ciftiTools)
library(devtools)
use_build_ignore('R/diagnosticICA.R')
use_build_ignore('R/EM_diagnosticICA.R')
library(roxygen2)
roxygenize()
library(BayesfMRI)
if(!is.list(template_mean)) stop('template_mean must be a list')
library(roxygen2)
roxygenize()
library(BayesfMRI)
roxygenize()
roxygenize()
roxygenize()
roxygenize()
roxygenize()
library(BayesfMRI)
roxygenize()
roxygenize()
library(BayesfMRI)
roxygenize()
library(BayesfMRI)
load('~/tmp.Rdata')
library(matrixStats) #colVars
library(spatstat) #blur (in smooth_dev)
#library(devtools)
#install_github('mandymejia/BayesfMRI')
library(BayesfMRI)
setwd('~/Dropbox/RESEARCH/DiagnosticICA/simulation')
source('sim_funs.R')
Q
ii
dim(Dat_ii)
BOLD = Dat_ii[1:nt,]
scale=FALSE
verbose=TRUE
maxQ=Q
maxiter=100
epsilon=0.001
if(!is.list(template_mean)) stop('template_mean must be a list')
if(!is.list(template_var)) stop('template_var must be a list')
if(length(template_mean) != length(template_var)) stop('template_mean and template_var must have the same length')
G <- length(template_mean)
if(!is.matrix(BOLD)) stop('BOLD must be a matrix')
ntime <- nrow(BOLD) #length of timeseries
nvox <- ncol(BOLD) #number of data locations
L <- nrow(template_mean[[1]]) #number of ICs
cat(paste0('Length of timeseries: T = ',ntime,'\n'))
cat(paste0('Number of voxels/vertices: V = ',nvox,'\n'))
cat(paste0('Number of ICs: L = ',L,'\n'))
cat(paste0('Number of groups: G = ',G,'\n'))
#check that each element of template_mean and template_var is a matrix
#check that the dimensions of template_mean and template_var elements are ok
if(any(sapply(template_mean, is.matrix)==FALSE)) stop('Each element of template_mean must be an LxV matrix')
if(any(sapply(template_var, is.matrix)==FALSE)) stop('Each element of template_var must be an LxV matrix')
if(any(sapply(template_mean, dim)[1,] != L) | any(sapply(template_mean, dim)[2,] != nvox)) stop('Each element of template_mean must be an LxV matrix')
if(any(sapply(template_var, dim)[1,] != L) | any(sapply(template_var, dim)[2,] != nvox)) stop('Each element of template_mean must be an LxV matrix')
nt=200
BOLD = Dat_ii[1:nt,]
if(!is.list(template_mean)) stop('template_mean must be a list')
if(!is.list(template_var)) stop('template_var must be a list')
if(length(template_mean) != length(template_var)) stop('template_mean and template_var must have the same length')
G <- length(template_mean)
if(!is.matrix(BOLD)) stop('BOLD must be a matrix')
ntime <- nrow(BOLD) #length of timeseries
nvox <- ncol(BOLD) #number of data locations
L <- nrow(template_mean[[1]]) #number of ICs
cat(paste0('Length of timeseries: T = ',ntime,'\n'))
cat(paste0('Number of voxels/vertices: V = ',nvox,'\n'))
cat(paste0('Number of ICs: L = ',L,'\n'))
cat(paste0('Number of groups: G = ',G,'\n'))
#check that each element of template_mean and template_var is a matrix
#check that the dimensions of template_mean and template_var elements are ok
if(any(sapply(template_mean, is.matrix)==FALSE)) stop('Each element of template_mean must be an LxV matrix')
if(any(sapply(template_var, is.matrix)==FALSE)) stop('Each element of template_var must be an LxV matrix')
if(any(sapply(template_mean, dim)[1,] != L) | any(sapply(template_mean, dim)[2,] != nvox)) stop('Each element of template_mean must be an LxV matrix')
if(any(sapply(template_var, dim)[1,] != L) | any(sapply(template_var, dim)[2,] != nvox)) stop('Each element of template_mean must be an LxV matrix')
#check that the number of data locations (nvox), time points (ntime) and ICs (L) makes sense
if(ntime > nvox) warning('More time points than voxels. Are you sure?')
if(L > nvox) stop('The arguments you supplied suggest that you want to estimate more ICs than you have data locations.  Please check the orientation and size of template_mean, template_var and BOLD.')
if(L > ntime) stop('The arguments you supplied suggest that you want to estimate more ICs than you have time points.  Please check the orientation and size of template_mean, template_var and BOLD.')
if(round(maxiter) != maxiter | maxiter <= 0) stop('maxiter must be a positive integer')
#check that maxQ makes sense
if(!is.null(maxQ)){ if(round(maxQ) != maxQ | maxQ <= 0) stop('maxQ must be NULL or a round positive number') }
if(is.null(maxQ)) maxQ <- ntime
if(maxQ < L){
warning('maxQ must be at least L.  Setting maxQ=L.')
maxQ <- L
}
if(maxQ > ntime){
warning('maxQ must be no more than T.  Setting maxQ = T.')
maxQ <- ntime
}
if(class(scale) != 'logical' | length(scale) != 1) stop('scale must be a logical value')
# USE ORIGINAL DATA, SCALED, SINCE WE ARE ASSUMING NO NUISANCE COMPONENTS
BOLD3 <- scale_BOLD(BOLD, scale=scale) #center, and if scale=TRUE, scale
dat_list <- dim_reduce(BOLD3, L)
BOLD4 <- dat_list$data_reduced
H <- dat_list$H
Hinv <- dat_list$H_inv
C_diag <- dat_list$C_diag
C_diag <- C_diag * (dat_list$sigma_sq) #(nu^2)HH' in paper
template_mean_avg <- apply(abind(template_mean, along=3), c(1,2), mean)
#dat_DR <- dual_reg(BOLD3, template_mean[[1]])
dat_DR <- dual_reg(BOLD3, template_mean_avg)
HA <- H %*% dat_DR$A #apply dimension reduction
theta0 <- list(A = HA)
library(abind)
#initialize mixing matrix (use dual regression-based estimate for starting value)
template_mean_avg <- apply(abind(template_mean, along=3), c(1,2), mean)
#dat_DR <- dual_reg(BOLD3, template_mean[[1]])
dat_DR <- dual_reg(BOLD3, template_mean_avg)
HA <- H %*% dat_DR$A #apply dimension reduction
theta0 <- list(A = HA)
BOLD=BOLD4
ntime <- nrow(BOLD) #length of timeseries
nvox <- ncol(BOLD) #number of brain locations
L <- nrow(template_mean[[1]]) #number of ICs
G <- length(template_mean)
if(L > nvox) stop('Cannot estimate more ICs than brain locations.')
if(L > ntime) stop('Cannot estimate more ICs than time points.')
iter = 1
theta = theta0
success = 1
for(g in 1:G) template_var[[g]][template_var[[g]] < 1e-6] = 1e-6 #to prevent problems when inverting covariance
err = 1000 #large initial value for difference between iterations
num_smallvar <- rowSums(template_var[[g]] < 1e-6)
num_smallvar
num_smallvar <- sum(template_var[[g]] < 1e-6)
if(verbose) cat(paste0('Setting ',num_smallvar,'very small variance values in group ',g,' template to zero.'))
if(verbose) cat(paste0('Setting ',num_smallvar,' very small variance values in group ',g,' template to zero.'))
err = 1000 #large initial value for difference between iterations
return_MAP=FALSE
L <- nrow(BOLD)
nvox <- ncol(BOLD)
G <- length(template_mean)
#initialize new objects
theta_new = list(A = matrix(NA, L, L))
A_part1 = A_part2 = matrix(0, L, L) #two parts of product for A-hat (construct each looping over voxels)
A <- theta$A
C <- diag(C_diag)
C_inv <- diag(1/(C_diag))
At_Cinv <- t(A) %*% C_inv
At_Cinv_A <- At_Cinv %*% A
exp_part <- rep(NA, G)
pr_zy <- rep(NA, G)
for(g in 1:G){
#exp_part1 <- L*nvox*log(2*pi)
exp_part1 <- 0 #irrelevant as long as noninformative prior on z=g
exp_part2 <- sum(log(template_var[[g]])) #sum over v,ell
exp_part3 <- 0
for(v in 1:nvox){
y_v = BOLD[,v]
s0_gv = template_mean[[g]][,v]
#mean and cov of y(v)|z
mu_yz_v <- A %*% s0_gv
Sigma_yz_v <- A %*% diag(template_var[[g]][,v]) %*% t(A) + C
exp_part3_v <- t(y_v - mu_yz_v) %*% solve(Sigma_yz_v) %*% (y_v - mu_yz_v)
exp_part3 <- exp_part3 + exp_part3_v
}
exp_part[g] <- -0.5 * (exp_part1 + exp_part2 + exp_part3[1])
}
g=1
exp_part
exp_part-exp_part[g]
exp(exp_part-exp_part[g])
pr_zy_inv_g <- sum(exp(exp_part-exp_part[g])) # ( (e^M1 + e^M2 + e^M3) / e^M1 ) = e^(M1-M1) + e^(M2-M1) + e^(M3-M1) = 1 + e^(M2-M1) + e^(M3-M1)  <-- If any e^(Mk-Mg) Inf, the inverse will be zero so p(z=g|y)=0
1/pr_zy_inv_g
pr_zy_inv_g
for(g in 1:G){
pr_zy_inv_g <- sum(exp(exp_part-exp_part[g])) # ( (e^M1 + e^M2 + e^M3) / e^M1 ) = e^(M1-M1) + e^(M2-M1) + e^(M3-M1) = 1 + e^(M2-M1) + e^(M3-M1)  <-- If any e^(Mk-Mg) Inf, the inverse will be zero so p(z=g|y)=0
pr_zy[g] <- 1/pr_zy_inv_g
}
tmp = pr_zy
if(is.infinite(exp(max(exp_part)-min(exp_part)))) {
#this is for two groups, need to generalize for G>2
pr_zy[which.max(exp_part)] <- 1
pr_zy[which.min(exp_part)] <- 0
} else {
exp_part <- exp_part - mean(exp_part) #this minimizes the most extreme magnitude in exp_part (to avoid )
prod_pr_yz <- exp(exp_part)
#compute p(z=g|y), g=1,...,G
denom <- sum((1/G)*prod_pr_yz) #can change 1/G for unequal prior probs
pr_zy <- (1/G)*prod_pr_yz / denom
}
pr_zy
tmp
pr_zy[1]
pr_zy[2]
pr_zy[1]==1
pr_zy[2]==0
g
exp_part-exp_part[g]
exp(exp_part-exp_part[g])
pr_zy_inv_g <- sum(exp(exp_part-exp_part[g])) # ( (e^M1 + e^M2 + e^M3) / e^M1 ) = e^(M1-M1) + e^(M2-M1) + e^(M3-M1) = 1 + e^(M2-M1) + e^(M3-M1)  <-- If any e^(Mk-Mg) Inf, the inverse will be zero so p(z=g|y)=0
1/pr_zy_inv_g
any(pr_zy==1)
sum(pr_zy[pr_zy!=1])
1-sum(pr_zy[pr_zy!=1])
(1-sum(pr_zy[pr_zy!=1]))==1
#fix numerical issues with very small values (values very close to 1 are numerically equal to 1, while values very close to zero are not)
if(any(pr_zy==1)){
pr_zy[pr_zy!=1] <- 0
}
pr_zy
