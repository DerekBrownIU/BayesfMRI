image(A_all[[1]])
install.packages(c("forecast", "leaps"))
# install.packages("leaps")
library(leaps)
library(dplyr)
help(leaps)
# load and look at your data!
toyota.corolla.df <- read.csv("~/Downloads/ToyotaCorolla.csv")
car.df <- toyota.corolla.df
glimpse(car.df)
# use first 1000 rows of data
car.df <- car.df[1:1000, ]
# select variables for regression
selected.var <- c(3, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18)
car.df <- select(car.df, Price, Age_08_04, KM, Fuel_Type,HP,Met_Color,Automatic,CC, Doors,Quarterly_Tax,Weight)
glimpse(car.df)
set.seed(1)  # set seed for reproducing the partition
train.index <- sample(c(1:1000), 600)
train.df <- car.df[train.index,]
valid.df <- car.df[-train.index,]
# use lm() to run a linear regression of Price on all 11 predictors in the
# training set.
# use . after ~ to include all the remaining columns in train.df as predictors.
car.lm <- lm(Price ~ ., data = train.df)
#  use options() to ensure numbers are not displayed in scientific notation.
# options(scipen = 999)
summary(car.lm)
#### Table 6.4
library(forecast)
# use predict() to make predictions on a new set.
car.lm.pred <- predict(car.lm, valid.df)
# use accuracy() to compute common accuracy measures.
accuracy(car.lm.pred, valid.df$Price)
some.residuals <- valid.df$Price[1:20] - car.lm.pred[1:20]
data.frame("Predicted" = car.lm.pred[1:20], "Actual" = valid.df$Price[1:20], "Residual" = some.residuals)
library(ggplot2)
ggplot(lm(Price ~ ., data = train.df)) +
geom_point(aes(x=.fitted, y=.resid))
plot(car.lm)
car.lm.pred <- predict(car.lm, valid.df)
all.residuals <- valid.df$Price - car.lm.pred
length(all.residuals[which(all.residuals > -1406 & all.residuals < 1406)])/400
hist(all.residuals, breaks = 25, xlab = "Residuals", main = "")
# use regsubsets() in package leaps to run an exhaustive search.
# unlike with lm, categorical predictors must be turned into dummies manually.
# use regsubsets() in package leaps to run an exhaustive search.
# unlike with lm, categorical predictors must be turned into dummies manually.
library(leaps)
search <- regsubsets(Price ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
method = "exhaustive")
help("regsubsets")
dim(train.df)[2]
sum <- summary(search)
# show models
sum$which
# show metrics
data.frame(rsq = sum$rsq, adjr2 = sum$adjr2, cp=sum$cp)
#### Table 6.6
# use step() to run stepwise regression.
car.lm.step <- step(car.lm, direction = "backward")
summary(car.lm.step)  # Which variables were dropped?
## QUESTION FOR YOU:  Which variables were dropped?
car.lm.step.pred <- predict(car.lm.step, valid.df)
accuracy(car.lm.step.pred, valid.df$Price)
accuracy(car.lm.pred, valid.df$Price)
#### Table 6.7
# create model with no predictors
car.lm.null <- lm(Price~1, data = train.df)
# use step() to run forward regression.
car.lm.step <- step(car.lm.null, scope=list(lower=car.lm.null, upper=car.lm), direction = "forward")
summary(car.lm.step)  # Which variables were added?
car.lm.step.pred <- predict(car.lm.step, valid.df)
accuracy(car.lm.step.pred, valid.df$Price)
#### Table 6.8
# use step() to run stepwise regression.
car.lm.step <- step(car.lm, direction = "both")
summary(car.lm.step)  # Which variables were dropped/added?
car.lm.step.pred <- predict(car.lm.step, valid.df)
accuracy(car.lm.step.pred, valid.df$Price)
summary(search)
